{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>ChromaDB Data Pipes is a collection of tools to build data pipelines for Chroma DB, inspired by the Unix philosophy of \" do one thing and do it well\".</p> <p>Roadmap:</p> <ul> <li>\u2705 Integration with LangChain \ud83e\udd9c\ud83d\udd17</li> <li>\ud83d\udeab Integration with LlamaIndex \ud83e\udd99</li> <li>\u2705 Support more than <code>all-MiniLM-L6-v2</code> as embedding functions (head over   to Embedding Processors for more info)</li> <li>\ud83d\udeab Multimodal support</li> <li>\u267e\ufe0f Much more!</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install chromadb-data-pipes\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Get help:</p> <pre><code>cdp --help\n</code></pre> <p>.env Files</p> <p>CDP supports loading environment variables from <code>.env</code> files. You can create a <code>.env</code> file in the root of your project (or from wherever you run CDP commands) and add environment-specific variables on new lines in the form of <code>NAME=VALUE</code>.</p>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<p>This is a short list of use cases to evaluate whether this is the right tool for your needs:</p> <ul> <li>Importing large datasets from local documents (PDF, TXT, etc.), from HuggingFace, from local persisted Chroma DB or   even another remote Chroma DB.</li> <li>Exporting large dataset to HuggingFace or any other dataformat supported by the library (if your format is not   supported, either implement it in a small function or open an issue)</li> <li>Create a dataset from your data that you can share with others (including the embeddings)</li> <li>Clone Collection with different embedding function, distance function, and other HNSW fine-tuning parameters</li> <li>Re-embed documents in a collection with a different embedding function</li> <li>Backup your data to a <code>jsonl</code> file</li> <li>Use other existing unix or other tools to transform your data after exporting from or before importing into Chroma DB</li> </ul>"},{"location":"#importing","title":"Importing","text":"<p>Import data from HuggingFace Datasets to <code>.jsonl</code> file:</p> <pre><code>cdp ds-get \"hf://tazarov/chroma-qna?split=train\" &gt; chroma-qna.jsonl\n</code></pre> <p>Import data from HuggingFace Datasets to Chroma DB Server:</p> <p>The below command will import the <code>train</code> split of the given dataset to Chroma chroma-qna <code>chroma-qna</code> collection. The collection will be created if it does not exist and documents will be upserted.</p> <pre><code>cdp ds-get \"hf://tazarov/chroma-qna?split=train\" | cdp import \"http://localhost:8000/chroma-qna\" --upsert --create\n</code></pre> <p>Importing from a directory with PDF files into Local Persisted Chroma DB:</p> <pre><code>cdp ds-get sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 | cdp embed --ef default | cdp import \"file://chroma-data/my-pdfs\" --upsert --create\n</code></pre> <p>Note</p> <p>The above command will import the first PDF file from the <code>sample-data/papers/</code> directory, chunk it into 500 word chunks, embed each chunk and import the chunks to the <code>my-pdfs</code> collection in Chroma DB.</p>"},{"location":"#exporting","title":"Exporting","text":"<p>Export data from Local Persisted Chroma DB to <code>.jsonl</code> file:</p> <p>The below command will export the first 10 documents from the <code>chroma-qna</code> collection to <code>chroma-qna.jsonl</code> file.</p> <pre><code>cdp export \"file://chroma-data/chroma-qna\" --limit 10 &gt; chroma-qna.jsonl\n</code></pre> <p>Export data from Local Persisted Chroma DB to <code>.jsonl</code> file with filter:</p> <p>The below command will export data from local persisted Chroma DB to a <code>.jsonl</code> file using a <code>where</code> filter to select the documents to export.</p> <pre><code>cdp export \"file://chroma-data/chroma-qna\" --where '{\"document_id\": \"123\"}' &gt; chroma-qna.jsonl\n</code></pre> <p>Export data from Chroma DB Server to HuggingFace Datasets:</p> <p>The below command will export the first 10 documents with offset 10 from the <code>chroma-qna</code> collection to HuggingFace Datasets <code>tazarov/chroma-qna</code> dataset. The dataset will be uploaded to HF.</p> <p>Note</p> <p>Make sure you have <code>HF_TOKEN=hf_....</code> environment variable set. If you want your dataset to be private, add <code>--private</code> flag to the <code>cdp ds-put</code> command.</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" --limit 10 --offset 10 | cdp ds-put \"hf://tazarov/chroma-qna-modified\"\n</code></pre> <p>To export a dataset to a file, use <code>--uri</code> with <code>file://</code> prefix:</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" --limit 10 --offset 10 | cdp ds-put \"file://chroma-qna\"\n</code></pre> <p>Note</p> <p>The file is  relative to the current working directory.</p>"},{"location":"#processing","title":"Processing","text":"<p>Copy collection from one Chroma collection to another and re-embed the documents:</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" | cdp embed --ef default | cdp import \"http://localhost:8000/chroma-qna-def-emb\" --upsert --create\n</code></pre> <p>Embeddings Processor</p> <p>See Embedding Processors for more info about supported embedding functions.</p> <p>Import dataset from HF to Local Persisted Chroma and embed the documents:</p> <pre><code>cdp ds-get \"hf://tazarov/ds2?split=train\" | cdp embed --ef default | cdp import \"file://chroma-data/chroma-qna-def-emb-hf\" --upsert --create\n</code></pre> <p>Chunk Large Documents:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500\n</code></pre>"},{"location":"#misc","title":"Misc","text":"<p>Count the number of documents in a collection:</p> <pre><code>cdp export \"http://localhost:8000/chroma-qna\" | wc -l\n</code></pre>"},{"location":"auth/","title":"Authentication","text":"<p>When importing or exporting data from remote Chroma instance, you may need to authenticate your API requests. CDP supports auth via URL or env vars.</p>"},{"location":"auth/#url","title":"URL","text":"<p>Basic Auth:</p> <pre><code>cdp import/export http://admin:admin@localhost:8000/my_collection\n</code></pre> <p>Token Auth:</p> <pre><code>cdp import/export http://__auth_token__:chr0ma-t0k3n@localhost:8000/my_collection\n</code></pre> <p>X-Chroma Token Auth:</p> <pre><code>cdp import/export http://__x_chroma_token__:chr0ma-t0k3n@localhost:8000/my_collection\n</code></pre>"},{"location":"auth/#environment-variables","title":"Environment Variables","text":"<p>Basic Auth:</p> <pre><code>export CHROMA_BASIC_AUTH=admin:admin\ncdp import/export http://localhost:8000/my_collection\n</code></pre> <p>Token Auth:</p> <pre><code>export CHROMA_TOKEN_AUTH=chr0ma-t0k3n\ncdp import/export http://localhost:8000/my_collection\n</code></pre> <p>X-Chroma Token Auth:</p> <pre><code>export CHROMA_XTOKEN_AUTH=chr0ma-t0k3n\ncdp import/export http://localhost:8000/my_collection\n</code></pre>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#embeddableresource","title":"EmbeddableResource","text":"<p>ChromaDB Data Pipes operates over a structure called EmbeddableResource. The structure is a closely related to ChromaDB Documents. All core components of the library will either, produce, consume, or transform EmbeddableResources.</p> <p>Each embeddable resource has the following properties:</p> <ul> <li><code>id</code> - unique identifier of the resource</li> <li><code>metadata</code> - metadata of the resource</li> <li><code>embedding</code> - embedding of the resource</li> </ul> <p>For text resource we use <code>EmbeddableTextResource</code> which adds the following properties:</p> <ul> <li><code>text_chunk</code> - text of the resource</li> </ul> <p>Evolution</p> <p>We plan to evolve the EmbeddableResource structure to support more types of resources, such as images, audio, video,</p>"},{"location":"concepts/#producer","title":"Producer","text":"<p>Generates a stream of data to a file or stdout.</p> <p>The source of the data is implementation dependent, HF datasets, ChromaDB, file etc.</p>"},{"location":"concepts/#consumer","title":"Consumer","text":"<p>Consumes a stream of data from a file or stdin.</p>"},{"location":"concepts/#processor","title":"Processor","text":"<p>Consumes a stream of data from a file or stdin and processes it by some criteria. Produces a stream of data to a file or stdout.</p>"},{"location":"concepts/#pipeline","title":"Pipeline","text":"<p>Reusable set of producer, consumer, filter, and transformer.</p> <p>WIP</p> <p>This is a work in progress. Stay tuned for more updates.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/","title":"CLI Naming Conventions","text":"<p>Date: 2024-01-20 Tags: cli, naming, conventions Authors: @tazarov</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to be mindful of the naming conventions we use for the CLI. We want to be consistent and clear about the commands we use and the order of the arguments.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#considered-options","title":"Considered Options","text":"<ul> <li>Extensive use of subcommands</li> <li>Ecosystem centric naming</li> </ul>"},{"location":"adr/ADR-20240120-CLI-Naming/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Ecosystem centric naming\", because it will make the CLI more aligned with the ecosystem thus reducing the cognitive load on users.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-import-and-export-for-chroma","title":"The use of <code>import</code> and <code>export</code> for Chroma","text":"<p>This is a toolchain in the chroma ecosystem and we want to be very explicit about the use of import and export. They must convey the core idea behind CDP, getting data in and out of Chroma.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-embed","title":"The use of <code>embed</code>","text":"<p>A core concept around vector databases is embedding. We want to make the embedding a first class citizen of CDP this is why we choose to have embed as a top level command.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-chunk","title":"The use of <code>chunk</code>","text":"<p>Another core concept around vector databases and embedding models is context window. Each embedding model has a specific context window and sometimes the embedding models, make a trade-off to truncate data for better usability rather than warning or preventing the user from using the model with an oversized context window. To that end chunking (of mostly text data) has become a core concept in the ecosystem. We want to convey that by elevating the chunk command to a top level command.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#the-use-of-ds-get-and-ds-put","title":"The use of <code>ds-get</code> and <code>ds-put</code>","text":"<p>Working with datasets is another core concept in the econsystem. While we currently only plan to support HuggingFace datasets, we want to also put the datasets front and center in CDP CLI. This is why we choose to have <code>ds-get</code> and <code>ds-put</code> as top level commands.</p>"},{"location":"adr/ADR-20240120-CLI-Naming/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/ADR-20240120-CLI-Naming/#extensive-use-of-subcommands","title":"Extensive use of subcommands","text":"<ul> <li>Good, because it allows for a more natural grouping of commands</li> <li>Good, because it will solve a top-level command sprawl</li> <li>Bad, because it will put a burden on the user to remember and navigate the subcommands</li> </ul>"},{"location":"adr/ADR-20240120-CLI-Naming/#ecosystem-centric-naming","title":"Ecosystem centric naming","text":"<ul> <li>Good, because it will make the CLI more consistent with the ecosystem</li> <li>Good, because it reduces the cognitive load on users by aligning with expectations set by the ecosystem</li> <li>Bad, because it will increase the number of top-level commands</li> </ul>"},{"location":"processors/chunking/","title":"Chunking","text":""},{"location":"processors/chunking/#usage","title":"Usage","text":"<pre><code>cdp imp url &lt;url&gt; [flags]\n</code></pre> <p>The following example will chunk the document into 500 character chunks and print the chunks to stdout. We will also add (<code>-a</code> option) the offset position of each chunk within the document as metadata <code>start_index</code>.</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 -a\n</code></pre> <p>Alternatively you can chunk from an input <code>jsonl</code> file:</p> <pre><code>cdp imp pdf sample-data/papers/ | grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 &gt; chunk.jsonl\n</code></pre> <p>jsonl format</p> <p>It is expected that the <code>jsonl</code> file contains <code>chroma_dp.EmbeddableTextResource</code> objects (one per line).</p> <pre><code>cdp chunk -s 500 --in chunk.jsonl\n</code></pre> <p>Help</p> <p>Run <code>cdp chunk --help</code> for more information.</p>"},{"location":"processors/cleaning/","title":"Cleaning Processors","text":""},{"location":"processors/cleaning/#clean-emoji","title":"Clean Emoji","text":"<p>Usage:</p> <pre><code>cdp tx emoji-clean [-m] [-|&lt;file&gt;]\n</code></pre> <p>Get Help</p> <p>Get help for the command with the following flag:</p> <pre><code>cdp tx emoji-clean --help\n</code></pre>"},{"location":"processors/cleaning/#example","title":"Example","text":"<p>The following example cleans the ChromaDB docs home page from emojis, including metadata.</p> <pre><code>cdp imp url https://docs.trychroma.com/ -d 1 | cdp tx emoji-clean -m\n</code></pre>"},{"location":"processors/embedding/","title":"Embedding Processors","text":""},{"location":"processors/embedding/#default-embedding-processor","title":"Default Embedding Processor","text":"<p>CDP comes with a default embedding processor that supports the following embedding functions:</p> <ul> <li>Default (<code>default</code>) - The default ChromaDB embedding function based on OnnxRuntime and MiniLM-L6-v2 model.</li> <li>OpenAI (<code>openai</code>) - OpenAI's text-embedding-ada-002 model.</li> <li>Cohere (<code>cohere</code>) - Cohere's embedding models.</li> <li>HuggingFrace (<code>hf</code>) - HuggingFace's embedding models.</li> <li>SentenceTransformers (<code>st</code>) - SentenceTransformers' embedding models.</li> <li>Ollama (<code>ollama</code>) - Ollama's embedding models.</li> </ul> <p>The embedding functions are based on ChromaDB's embedding functions.</p> <p>.env Files</p> <p>CDP supports loading environment variables from <code>.env</code> files. You can create a <code>.env</code> file in the root of your project (or from wherever you run CDP commands) and add environment-specific variables on new lines in the form of <code>NAME=VALUE</code>.</p>"},{"location":"processors/embedding/#usage","title":"Usage","text":""},{"location":"processors/embedding/#default","title":"Default","text":"<p>The below command will read a PDF files at the specified path, filter the output for a particular pdf (<code>grep</code>). Select the first document's page, chunk it to 500 characters, embed each chunk using Chroma's default (MiniLM-L2-v2) model. The resulting documents with embeddings will be written to <code>chroma-data.jsonl</code> file.</p> <pre><code>cdp imp pdf sample-data/papers/ | cdp chunk -s 500 | cdp embed --ef default &gt; chroma-data.jsonl\n</code></pre>"},{"location":"processors/embedding/#openai","title":"OpenAI","text":"<p>To use this embedding function, you need to install the <code>openai</code> python package.</p> <pre><code>pip install openai\n</code></pre> <p>OpenAI API Key</p> <p>You need to have an OpenAI API key to use this embedding function. You can get an API key by signing up for an account at OpenAI API Keys page. The API key must be exported as env variable <code>OPENAI_API_KEY=sk-xxxxxx</code>.</p> <p>OpenAI Embedding Models</p> <p>By default, if not specified, the <code>text-embedding-ada-002</code> model is used. You can pass in an optional <code>--model=text-embedding-3-small</code> argument or env variable <code>GEMINI_MODEL_NAME=text-embedding-3-large</code> , which lets you choose which OpenAI embeddings model to use.</p> <p>The below command will read a PDF files at the specified path, filter the output for a particular pdf (<code>grep</code>). Select the first document's page, chunk it to 500 characters, embed each chunk using OpenAI's text-embedding-ada-002 model.</p> <pre><code>export OPENAI_API_KEY=sk-xxxxxx\ncdp imp pdf sample-data/papers/ |grep \"2401.02412.pdf\" | head -1 | cdp chunk -s 500 | cdp embed --ef openai\n</code></pre>"},{"location":"processors/embedding/#cohere","title":"Cohere","text":"<p>To use this embedding function, you need to install the <code>cohere</code> python package.</p> <pre><code>pip install cohere\n</code></pre> <p>Cohere API Key</p> <p>You need to have a Cohere API key to use this embedding function. You can get an API key by signing up for an account at Cohere. The API key must be exported as env variable <code>COHERE_API_KEY=x4q...</code>.</p> <p>Cohere Embedding Models</p> <p>By default, if not specified, the <code>embed-english-v3.0</code> model is used. You can pass in an optional <code>--model=embed-english-light-v3.0</code> argument or env variable <code>COHERE_MODEL_NAME=embed-multilingual-v3.0</code> , which lets you choose which Cohere embeddings model to use. More about available models can be found at Cohere's API docs</p> <p>The below command will read a PDF files at the specified path, select the last document's page, chunk it to 100 characters, embed each chunk using Cohere's embed-english-light-v3.0 model.</p> <pre><code>export COHERE_API_KEY=x4q\nexport COHERE_MODEL_NAME=\"embed-english-light-v3.0\"\ncdp imp pdf sample-data/papers/ | tail -1 | cdp chunk -s 100 | cdp embed --ef cohere\n</code></pre>"},{"location":"processors/embedding/#huggingface","title":"HuggingFace","text":"<p>HF API Token</p> <p>You need to have a HuggungFace API token to use this embedding function. Create or use one from your tokens page. The API key must be exported as env variable <code>HF_TOKEN=hf_xxxx</code>.</p> <p>HF Embedding Models</p> <p>By default, if not specified, the <code>sentence-transformers/all-MiniLM-L6-v2</code> model is used. You can pass in an optional <code>--model=BAAI/bge-large-en-v1.5</code> argument or env variable <code>HF_MODEL_NAME=BAAI/bge-large-en-v1.5</code> , which lets you choose which Hugging Frace embeddings model to use.</p> <p>The below command will read a PDF files at the specified path, select the first two pages, chunk it to 150 characters, selects the last chunk and embeds the chunk using BAAI/bge-large-en-v1.5 model.</p> <pre><code>export HF_TOKEN=hf_xxxx\nexport HF_MODEL_NAME=\"BAAI/bge-large-en-v1.5\"\ncdp imp pdf sample-data/papers/ | head -2 | cdp chunk -s 150 | tail -1 | cdp embed --ef hf\n</code></pre>"},{"location":"processors/embedding/#sentencetransformers","title":"SentenceTransformers","text":"<p>To use this embedding function, you need to install the <code>sentence-transformers</code> python package.</p> <pre><code>pip install sentence-transformers\n</code></pre> <p>SentenceTransformers Embedding Models</p> <p>By default, if not specified, the <code>all-MiniLM-L6-v2</code> model is used. You can pass in an optional <code>--model=BAAI/bge-large-en-v1.5</code> argument or env variable <code>ST_MODEL_NAME=BAAI/bge-large-en-v1.5</code> , which lets you choose which Sentence Transformers embeddings model to use.</p> <p>The below command will read a PDF files at the specified path, select the first two pages, chunk it to 150 characters, selects the last chunk and embeds the chunk using BAAI/bge-small-en-v1.5 model.</p> <pre><code>export ST_MODEL_NAME=\"BAAI/bge-small-en-v1.5\"\ncdp imp pdf sample-data/papers/ | head -2 | cdp chunk -s 150 | tail -1 | cdp embed --ef st\n</code></pre>"},{"location":"processors/embedding/#google-generative-ai-embedding-gemini","title":"Google Generative AI Embedding (Gemini)","text":"<p>To use Google Generative AI Embedding (Gemini) function, you need to install the <code>google-generativeai</code> python package.</p> <pre><code>pip install google-generativeai\n</code></pre> <p>Google API Key</p> <p>You need to have a Google API key to use this embedding function. To manage your keys go to Maker Suite. The API key must be exported as env variable <code>GEMINI_API_KEY=xxxx</code>.</p> <p>Models</p> <p>By default, if not specified, the <code>models/embedding-001</code> model is used. You can pass in an optional <code>--model=models/embedding-001</code> argument or env variable <code>GEMINI_MODEL_NAME=models/embedding-001</code>, which lets you choose which Gemini embeddings model to use.</p> <p>Task Type</p> <p>The embedding function also supports task type parameter. By default we use <code>RETRIEVAL_DOCUMENT</code>, For more details visit Gemini API Docs.</p> <p>The below command will read a PDF files at the specified path, select the first two pages, chunk it to 150 characters, selects the last chunk and embeds the chunk using <code>models/embedding-001</code> model.</p> <pre><code>cdp imp pdf sample-data/papers/ | head -2 | cdp chunk -s 150 | tail -1 | cdp embed --ef gemini\n</code></pre>"},{"location":"processors/embedding/#ollama-embeddings","title":"Ollama Embeddings","text":"<p>To use Ollama embedding function, you need to run an Ollama server see instructions on official Ollama GH repo</p> <p>Models</p> <p>By default, if not specified, the <code>chroma/all-minilm-l6-v2-f32</code> model is used. You can pass in an optional <code>--model=nomic-embed-text</code> argument or env variable <code>OLLAMA_MODEL_NAME=nomic-embed-text</code>, which lets you choose which Ollama embeddings model to use.</p> <p>Embedding URL</p> <p>By default the embedding function will try to connect to Ollama server running on <code>http://localhost:11434/api/embeddings</code> endpoint. If you whish to override that you can export an env var <code>OLLAMA_EMBED_URL</code></p> <p>The below command will read a PDF files at the specified path, select the first two pages, chunk it to 150 characters, selects the last chunk and embeds the chunk using <code>nomic-embed-text</code> model.</p> <pre><code>cdp imp pdf sample-data/papers/ | head -2 | cdp chunk -s 150 | tail -1 | cdp embed --ef ollama --model=chroma/all-minilm-l6-v2-f32\n</code></pre>"},{"location":"processors/id/","title":"ID Generation","text":"<p>CDP provides a number of ID generation strategies that can be used to generate or regenerate IDs for a given dataset.</p> <p>The following strategies are available:</p> <ul> <li>UUID - IDS are generated using the UUID</li> <li>ULID - IDS are generated using the ULID</li> <li>Document Hash - IDS are generated using the document hash (SHA256)</li> <li>Random Hash - IDS are generated using a random hash (SHA256)</li> <li>Expression - IDS are generated using a Jinja2 expression</li> </ul>"},{"location":"processors/id/#usage","title":"Usage","text":"<p>Help</p> <p>Run <code>cdp id --help</code> for more information.</p>"},{"location":"processors/id/#uuid","title":"UUID","text":"<p>This strategy generates a unique ID based on UUIDv4.</p> <pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp id --uuid | jq '.id'\n</code></pre> <p>Returns:</p> <pre><code>\"5bf1d91b-817c-47a2-a6cb-94894c1b42c3\"\n</code></pre>"},{"location":"processors/id/#ulid","title":"ULID","text":"<p>This strategy generates a unique ID based on ULID.</p> <pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp id --ulid | jq '.id'\n</code></pre> <p>Returns:</p> <pre><code>\"01HMR7V5MMVD3Q1PA5PPSF0FA1\"\n</code></pre>"},{"location":"processors/id/#document-hash","title":"Document Hash","text":"<p>This strategy generates unique IDs based on the document hash (SHA256).</p> <pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp id --doc-hash | jq '.id'\n</code></pre> <p>Returns:</p> <pre><code>\"3143643f8520f32f7b04fff2cd524acbe32ef989b2bd6cc89d687743a909bfa6\"\n</code></pre>"},{"location":"processors/id/#random-hash","title":"Random Hash","text":"<p>This strategy generates unique IDs based on a random hash (SHA256).</p> <pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp id --random-hash | jq '.id'\n</code></pre> <p>Returns:</p> <pre><code>\"f4d7bc16f4ddbd080b08c4836efa93ed51e85ea1289df6c5851e261893e6ad52\"\n</code></pre>"},{"location":"processors/id/#expression","title":"Expression","text":"<p>Generates ID based on provided Jinja2 expression. The following variables are available for use in the expression:</p> <ul> <li><code>metadata</code> - the metadata for the document</li> <li><code>text_chunks</code> - the text chunks for the document</li> <li><code>id</code> - existing ID for the document</li> <li><code>embedding</code> - the embedding for the document</li> <li><code>uuid</code> - function that generates a UUID (example usage <code>{{uuid()}}</code>)</li> <li><code>ulid</code> - function that generates a ULID (example usage <code>{{ulid()}}</code>)</li> </ul> <pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp id --expr '{{ulid()}}-{{metadata.title}}' | jq '.id'\n</code></pre> <p>Returns:</p> <pre><code>\"01HMR8308TMG80XYV9CHNAF0A3-Animalia (book)\"\n</code></pre>"},{"location":"processors/metadata/","title":"Metadata","text":"<p>The library offers a way to easily manipulate metadata values by adding or updating existing metadata keys as well as removing metadata keys from the metadata dictionary.</p>"},{"location":"processors/metadata/#usage","title":"Usage","text":"<p>Deprecation</p> <pre><code>We are removing `-m` and `--meta` flags as they break from our end goal of consistent and non-ambiguous CLI flags.\n</code></pre> <pre><code>cdp meta [-a key=value] [-k key_to_remove] [-o]\n</code></pre> <p>To add or update metadata key use <code>-a</code> flag with a <code>key=value</code> pair. The key is always assumed to be a string. The value is processed as follows - boolean value (true/false), float value, integer value. If the value cannot be parsed as any of the above types, it is assumed to be a string. If the value is with escaped single-quotes <code>'</code> (e.g. <code>\\'value\\'</code>), the escaped single-quotes are removed and the value treated as a string.</p> <p>To remove metadata keys use <code>-k</code> flag. If the key does not exist in the metadata dictionary, it is ignored.</p> <p>Help</p> <p>Run <code>cdp meta --help</code> for more information.</p>"},{"location":"processors/metadata/#add-metadata-keys","title":"Add metadata keys","text":"<pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp meta -a bool_value=false -a int_value=1 -a float_value=1.1 -a escaped_value=\\'true\\' | jq .metadata\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"title\": \"Animalia (book)\",\n  \"bool_value\": false,\n  \"int_value\": 1,\n  \"float_value\": 1.1,\n  \"escaped_value\": \"true\"\n}\n</code></pre>"},{"location":"processors/metadata/#overwrite-metadata-keys","title":"Overwrite metadata keys","text":"<pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp meta -a title=\"New Title\" -o | jq .metadata\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"title\": \"New Title\"\n}\n</code></pre>"},{"location":"processors/metadata/#remove-metadata-keys","title":"Remove metadata keys","text":"<pre><code>cat sample-data/metadata/metadata.jsonl | head -1 |  cdp meta -a bool_value=false -a int_value=1 | cdp meta -k int_value | jq .metadata\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"title\": \"Animalia (book)\",\n  \"bool_value\": false\n}\n</code></pre>"},{"location":"processors/metadata/#templating","title":"Templating","text":"<p>It is also possible to pass template values to metadata values. The template values must be valid JINJA2 templates.</p> <pre><code>cat sample-data/metadata/metadata.jsonl | head -1 | cdp meta -a extracted_title=\"{{ metadata.title | upper}}\" | jq .metadata\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"title\": \"Animalia (book)\",\n  \"extracted_title\": \"ANIMALIA (BOOK)\"\n}\n</code></pre> <p>The following context vars and functions are available in the template:</p> <ul> <li><code>metadata</code>: the metadata dictionary of the original doc</li> <li><code>text_chunk</code>: the text chunk of the original doc</li> <li><code>now</code>: the current datetime. Example usage <code>{{ now }}</code></li> <li><code>date</code>: Date in specified format, if not specified epoch time is returned. Example usage <code>{{ '%Y-%m-%d'| date }}</code></li> <li>All the default jinja2 filters.</li> </ul>"},{"location":"producers/csv/","title":"CSV Files","text":""},{"location":"producers/csv/#default-csv-file-generator","title":"Default CSV File Generator","text":"<p>The default CSV file generator reads a single CSV file provided as argument to <code>cdp imp csv</code> command.</p> <p>Practical example:</p> <pre><code> cdp imp csv sample-data/csv/employees_with_resumes.csv  \\\n  --meta-features Name \\\n  --meta-features Department \\\n  --doc-feature Resume \\\n  | tail -1 \\\n  | cdp embed --ef default\n</code></pre> <p>The above command will read the CSV file <code>sample-data/csv/employees_with_resumes.csv</code> and will use the columns <code>Name</code> and <code>Department</code> as metadata features and the column <code>Resume</code> as document feature. The output of the command is piped to <code>cdp embed</code> which will embed the document feature using the default embedding function.</p>"},{"location":"producers/csv/#options","title":"Options","text":"<ul> <li><code>--meta-features</code> - a list of columns to be used as metadata features (specified multiple times, once per column)</li> <li><code>--doc-feature</code> - a column to be used as document feature. If unspecified, the entire row is used as document feature.</li> <li><code>--batch-size</code> - the number of rows to sbe read at once. Defaults to 100.</li> <li><code>--delimiter</code> - the delimiter used in the CSV file. Defaults to <code>,</code>.</li> <li><code>--quotechar</code> - the quote character used in the CSV file. Defaults to <code>\"</code>.</li> </ul> <p>Help</p> <p>Run <code>cdp imp csv --help</code> for more information.</p>"},{"location":"producers/pdf/","title":"PDFs","text":""},{"location":"producers/pdf/#pypdf","title":"PyPDF","text":"<p>Reading a dir with PDF files to stdout:</p> <pre><code>cdp imp pdf sample-data/papers/\n</code></pre> <p>Help</p> <p>Run <code>cdp imp pdf --help</code> for more information.</p>"},{"location":"producers/text/","title":"Text Files","text":""},{"location":"producers/text/#default-text-file-generator","title":"Default Text File Generator","text":"<p>Reading a dir with text files to stdout:</p> <pre><code>cdp imp txt sample-data/text/ | head -1 | cdp chunk -s 100 | tail -1 | cdp embed --ef default\n</code></pre> <p>Help</p> <p>Run <code>cdp imp txt --help</code> for more information.</p>"},{"location":"producers/url/","title":"URL Importer","text":"<p>Imports data from a URL.</p>"},{"location":"producers/url/#default","title":"Default","text":""},{"location":"producers/url/#usage","title":"Usage","text":"<pre><code>cdp imp url &lt;url&gt; [flags]\n</code></pre> <p>Get Help</p> <p>Get help for the command with the following flag:</p> <pre><code>cdp imp url --help\n</code></pre> <p>URLs with binary data</p> <p>Currently the URL importer does not support URLs with binary data, such as PDFs or other binary files. Need this? Raise an issue here.</p> <p>The following example imports data from ChromaDB documentation with max depth 2.</p> <pre><code>cdp imp url https://docs.trychroma.com/embeddings -d 2\n</code></pre>"},{"location":"producers/url/#advanced-usage","title":"Advanced Usage","text":"<p>The following example imports data from ChromaDB documentation with max depth 3, chunks the data into 512 byte chunks, cleans the data of emojis, and embeds the data using the default embedding function.</p> <pre><code>cdp imp url https://docs.trychroma.com/ -d 3 | cdp chunk -s 512| cdp tx emoji-clean -m | cdp embed --ef default\n</code></pre>"}]}